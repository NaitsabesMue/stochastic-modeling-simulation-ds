{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 6 — Model Selection and Optimisation\n",
        "\n",
        "From fitted queueing models to simulation-based decisions with costs and confidence intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d90f6353",
      "metadata": {},
      "source": [
        "## Learning Objectives\n",
        "- Choose and test queue models against data/assumptions.\n",
        "- Simulate G/G/c systems (plain Python) with optional abandonment.\n",
        "- Show how structural misspecification (ignoring abandonment) biases conclusions.\n",
        "- Run scenario analysis and simple cost-based comparisons.\n",
        "- Practice designing robustness/diagnostics exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6281e816",
      "metadata": {},
      "source": [
        "## Notebook Roadmap\n",
        "1. Define a toy queueing system (no external data).\n",
        "2. Simulation helper for G/G/c with optional abandonment.\n",
        "3. Baseline runs (no abandonment) and metrics.\n",
        "4. Toy misspecification: true abandonment, naive model without abandonment.\n",
        "5. Scenario exploration (servers, demand multipliers).\n",
        "6. Optional cost comparison.\n",
        "7. Exercises for students."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f379a1ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "from typing import Callable, Tuple\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "\n",
        "RNG = np.random.default_rng\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46db5617",
      "metadata": {},
      "source": [
        "## Toy System Definition\n",
        "\n",
        "We simulate from a synthetic stationary system. Change parameters below to explore different behaviours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b515c022",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base toy parameters\n",
        "BASE_PARAMS = {\n",
        "    \"lambda_rate\": 0.9,   # arrival rate\n",
        "    \"delta\": 0.2,         # service shift (deterministic part)\n",
        "    \"theta\": 1.4,         # exponential tail rate\n",
        "    \"servers\": 2,         # baseline server count\n",
        "    \"run_time\": 2000.0,\n",
        "    \"warmup\": 200.0,\n",
        "}\n",
        "print(BASE_PARAMS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8982b4c1",
      "metadata": {},
      "source": [
        "## Simulation Helper (Optional Abandonment)\n",
        "\n",
        "Plain-Python discrete-event simulator for a G/G/c system with optional customer abandonment. Pass a patience sampler to enable reneging; leave as `None` for infinite patience.\n",
        "\n",
        "Patience sampler\n",
        "- Purpose: when provided, the simulator samples a patience duration for each arriving customer and computes a patience deadline = arrival_time + patience_duration. If the customer's service does not start before that deadline they abandon (reneging).\n",
        "- Signature: a callable of form `patience_sampler(rng) -> float`, where `rng` is a numpy.random.Generator and the returned float is a non‑negative time (same time units as `run_time`, `warmup`, service times).\n",
        "- Behaviour:\n",
        "    - If `patience_sampler is None` customers have infinite patience (no abandonments).\n",
        "    - If a sampled deadline < current time when a server frees, the customer is counted as abandoned and removed from the queue.\n",
        "    - Abandoned customers are included in the `abandoned` metric; for reporting the simulator records their wait up to the deadline (service time = 0) so they contribute to wait/system summaries appropriately.\n",
        "- Examples:\n",
        "    - Exponential patience with mean 1.0: `patience_sampler = lambda rng: rng.exponential(1.0)`\n",
        "    - Deterministic patience of 10 time units: `patience_sampler = lambda rng: 10.0`\n",
        "- Notes:\n",
        "    - Keep units consistent across arrival/service/patience times.\n",
        "    - To approximate “very patient” customers use a large constant or `None` for true infinite patience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c85fe6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def make_shifted_exp_sampler(delta: float, theta: float) -> Callable[[np.random.Generator], float]:\n",
        "    \"\"\"\n",
        "    Factory function to create a sampler for Shifted Exponential distribution.\n",
        "    Service time S = delta + X, where X ~ Exp(theta).\n",
        "    \n",
        "    Args:\n",
        "        delta: Minimum service time (shift).\n",
        "        theta: Rate parameter for the exponential component.\n",
        "    \"\"\"\n",
        "    def sampler(rng: np.random.Generator) -> float:\n",
        "        # rng.exponential(scale) uses scale = 1/rate\n",
        "        return delta + rng.exponential(1.0 / theta)\n",
        "    return sampler\n",
        "\n",
        "\n",
        "def simulate_queue(\n",
        "    lambda_rate: float,\n",
        "    service_sampler: Callable[[np.random.Generator], float],\n",
        "    c: int = 2,\n",
        "    run_time: float = 2000.0,\n",
        "    warmup: float = 200.0,\n",
        "    seed: int | None = None,\n",
        "    max_jobs: int = 200000,\n",
        "    patience_sampler: Callable[[np.random.Generator], float] | None = None,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Simulates a G/G/c queue with optional customer abandonment (reneging).\n",
        "    \n",
        "    This Discrete Event Simulation (DES) tracks the state of the queue over time,\n",
        "    handling arrivals, service completions, and customer patience.\n",
        "\n",
        "    Args:\n",
        "        lambda_rate: Arrival rate (Poisson process).\n",
        "        service_sampler: Function returning random service times.\n",
        "        c: Number of servers.\n",
        "        run_time: Total simulation duration.\n",
        "        warmup: Time to run before collecting statistics (transient removal).\n",
        "        seed: Random seed for reproducibility.\n",
        "        max_jobs: Safety limit on total departures to prevent infinite loops.\n",
        "        patience_sampler: Optional function returning random patience times.\n",
        "                          If provided, customers leave the queue if wait > patience.\n",
        "                          If None, customers have infinite patience.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing performance metrics (mean wait, utilization, etc.).\n",
        "    \"\"\"\n",
        "    rng = RNG(seed)\n",
        "    t = 0.0         # Current simulation clock\n",
        "    last_t = 0.0    # Time of the previous event (for integral calculation)\n",
        "    \n",
        "    # Generate the first arrival\n",
        "    next_arrival = rng.exponential(1.0 / lambda_rate)\n",
        "    \n",
        "    # Event Heap: Stores (completion_time, arrival_time, start_time, service_time)\n",
        "    # Ordered by completion_time (min-heap)\n",
        "    completions = [] \n",
        "    \n",
        "    # Queue: List of (arrival_time, patience_deadline)\n",
        "    # patience_deadline is the absolute time by which service MUST start to avoid abandonment.\n",
        "    queue = []        \n",
        "    \n",
        "    # State variables\n",
        "    busy = 0          # Number of servers currently busy\n",
        "    area_queue = 0.0  # Time-integrated queue length (for Lq)\n",
        "    area_busy = 0.0   # Time-integrated busy servers (for Utilization)\n",
        "    wait_samples = [] # Collected wait times\n",
        "    system_samples = [] # Collected system times (wait + service)\n",
        "    departures = 0    # Count of served customers\n",
        "    abandoned = 0     # Count of customers who reneged\n",
        "\n",
        "    while t < run_time and departures < max_jobs:\n",
        "        # --- 1. Determine Next Event ---\n",
        "        # The next event is either a new arrival or a service completion.\n",
        "        next_completion = completions[0][0] if completions else float('inf')\n",
        "        next_event = min(next_arrival, next_completion)\n",
        "        \n",
        "        # Check if simulation time is over\n",
        "        if next_event > run_time:\n",
        "            # Add final slice of statistics up to run_time\n",
        "            eff_start = max(last_t, warmup)\n",
        "            if run_time > eff_start:\n",
        "                dt = run_time - eff_start\n",
        "                area_queue += len(queue) * dt\n",
        "                area_busy += busy * dt\n",
        "            break\n",
        "\n",
        "        # --- 2. Update Statistics (Time Integration) ---\n",
        "        # We calculate the area under the curve for queue length and busy servers.\n",
        "        # Only accumulate stats if we are past the warmup period.\n",
        "        if next_event > warmup:\n",
        "            eff_start = max(last_t, warmup)\n",
        "            dt = next_event - eff_start\n",
        "            if dt > 0:\n",
        "                area_queue += len(queue) * dt\n",
        "                area_busy += busy * dt\n",
        "\n",
        "        # Advance clock\n",
        "        t = next_event\n",
        "        last_t = next_event\n",
        "\n",
        "        # --- 3. Process Event ---\n",
        "        if next_arrival <= next_completion:\n",
        "            # === ARRIVAL EVENT ===\n",
        "            \n",
        "            # Calculate patience deadline if the feature is enabled.\n",
        "            # Deadline = Current Time + Sampled Patience Duration.\n",
        "            patience_deadline = None\n",
        "            if patience_sampler is not None:\n",
        "                patience_deadline = t + patience_sampler(rng)\n",
        "            \n",
        "            if busy < c:\n",
        "                # Server available: Start service immediately.\n",
        "                busy += 1\n",
        "                s = service_sampler(rng)\n",
        "                completion_time = t + s\n",
        "                completions.append((completion_time, t, t, s))\n",
        "                completions.sort(key=lambda x: x[0])\n",
        "            else:\n",
        "                # All servers busy: Add to queue with deadline.\n",
        "                queue.append((t, patience_deadline))\n",
        "            \n",
        "            # Schedule next arrival\n",
        "            next_arrival = t + rng.exponential(1.0 / lambda_rate)\n",
        "        else:\n",
        "            # === COMPLETION EVENT ===\n",
        "            \n",
        "            # Free the server and record stats for the departing customer\n",
        "            completion_time, arrival_time, start_time, service_time = completions.pop(0)\n",
        "            busy -= 1\n",
        "            \n",
        "            if arrival_time >= warmup:\n",
        "                wait = start_time - arrival_time\n",
        "                system_time = wait + service_time\n",
        "                wait_samples.append(wait)\n",
        "                system_samples.append(system_time)\n",
        "                departures += 1\n",
        "            \n",
        "            # Assign the newly free server to the next customer in queue.\n",
        "            # CRITICAL: We must check for abandoned customers here.\n",
        "            while queue and busy < c:\n",
        "                arrival_time, patience_deadline = queue.pop(0) # FIFO\n",
        "                \n",
        "                # Check if the customer has already lost patience.\n",
        "                # If patience_deadline < current time 't', they left before we could serve them.\n",
        "                if patience_deadline is not None and patience_deadline < t:\n",
        "                    abandoned += 1\n",
        "                    \n",
        "                    # Record stats for abandoned customers too.\n",
        "                    # They waited until their deadline, then left (service time = 0).\n",
        "                    if arrival_time >= warmup:\n",
        "                        wait = patience_deadline - arrival_time\n",
        "                        wait_samples.append(wait)\n",
        "                        system_samples.append(wait)\n",
        "\n",
        "                    # This customer is gone; loop again to check the next person.\n",
        "                    continue\n",
        "                \n",
        "                # Customer is still waiting: Start service.\n",
        "                busy += 1\n",
        "                s = service_sampler(rng)\n",
        "                start_time = t\n",
        "                completion_time = t + s\n",
        "                completions.append((completion_time, arrival_time, start_time, s))\n",
        "                completions.sort(key=lambda x: x[0])\n",
        "\n",
        "    # --- 4. Finalize Metrics ---\n",
        "    horizon = max(run_time - warmup, 1.0)\n",
        "    return {\n",
        "        \"mean_wait\": float(np.mean(wait_samples)) if wait_samples else np.nan,\n",
        "        \"mean_system\": float(np.mean(system_samples)) if system_samples else np.nan,\n",
        "        \"lq_timeavg\": area_queue / horizon,\n",
        "        \"utilisation\": area_busy / (horizon * c),\n",
        "        \"departures\": departures,\n",
        "        \"abandoned\": abandoned,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_replications(\n",
        "    config: dict,\n",
        "    n_rep: int = 20,\n",
        "    run_time: float = 2000.0,\n",
        "    warmup: float = 200.0,\n",
        "    seed: int = 0,\n",
        "    patience_sampler: Callable[[np.random.Generator], float] | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs multiple independent replications of the simulation.\n",
        "    Useful for generating confidence intervals and smoothing out stochastic noise.\n",
        "    \"\"\"\n",
        "    sampler = make_shifted_exp_sampler(config[\"delta\"], config[\"theta\"])\n",
        "    rows = []\n",
        "    for r in range(n_rep):\n",
        "        rows.append(\n",
        "            simulate_queue(\n",
        "                lambda_rate=config[\"lambda_rate\"],\n",
        "                service_sampler=sampler,\n",
        "                c=config[\"servers\"],\n",
        "                run_time=run_time,\n",
        "                warmup=warmup,\n",
        "                seed=seed + r,\n",
        "                patience_sampler=patience_sampler,\n",
        "            )\n",
        "        )\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbc13f80",
      "metadata": {},
      "source": [
        "## Baseline (No Abandonment)\n",
        "\n",
        "Simulate the toy system with no abandonment and summarise performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840078ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_runs = run_replications(BASE_PARAMS, n_rep=30, run_time=BASE_PARAMS['run_time'], warmup=BASE_PARAMS['warmup'], seed=42, patience_sampler=None)\n",
        "\n",
        "summary = baseline_runs.mean().to_dict()\n",
        "print(\"Baseline metrics (no abandonment):\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k:12s} {v:8.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "766e72c8",
      "metadata": {},
      "source": [
        "\n",
        "## Toy Misspecification: Ignoring Abandonment\n",
        "\n",
        "Simulate data where customers have finite patience (some abandon) but then evaluate a model that assumes infinite patience. This illustrates how violating structural assumptions biases conclusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0167b8ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Ground Truth: Simulate a system where customers abandon ---\n",
        "# We define a patience distribution (Exponential) and run one long simulation\n",
        "# to get a sample path representing the \"real world\".\n",
        "print(\"--- 1. Ground Truth Simulation (with Abandonment) ---\")\n",
        "patience_mean = 1.0\n",
        "patience_sampler = lambda rng: rng.exponential(patience_mean)\n",
        "\n",
        "# Run a single, longer simulation to represent the \"true\" system behavior\n",
        "true_runs = run_replications(\n",
        "    BASE_PARAMS, n_rep=1, run_time=5000.0, warmup=500.0, seed=999, patience_sampler=patience_sampler\n",
        ")\n",
        "true_metrics = true_runs.iloc[0]\n",
        "\n",
        "# Calculate key metrics from this true system\n",
        "total_arrivals_est = true_metrics.departures + true_metrics.abandoned\n",
        "abandon_rate = true_metrics.abandoned / total_arrivals_est if total_arrivals_est > 0 else 0\n",
        "\n",
        "print(f\"True arrival rate (lambda): {BASE_PARAMS['lambda_rate']:.3f}\")\n",
        "print(f\"Observed departures:        {true_metrics.departures:.0f}\")\n",
        "print(f\"Observed abandonments:      {true_metrics.abandoned:.0f}\")\n",
        "print(f\"Estimated abandon rate:     {abandon_rate:.2%}\")\n",
        "print(f\"True mean wait (Wq):        {true_metrics.mean_wait:.4f}\\n\")\n",
        "\n",
        "\n",
        "# --- 2. Naive Model: Analyst ignores abandonment ---\n",
        "# The analyst only observes departures and incorrectly assumes this is the total arrival rate.\n",
        "# They build a model with no abandonment, using a lower, incorrect arrival rate.\n",
        "print(\"--- 2. Naive Model Simulation (Ignoring Abandonment) ---\")\n",
        "# The analyst underestimates the arrival rate by only counting completions.\n",
        "naive_lambda = BASE_PARAMS[\"lambda_rate\"] * (1 - abandon_rate)\n",
        "naive_cfg = {**BASE_PARAMS, \"lambda_rate\": naive_lambda}\n",
        "\n",
        "print(f\"Naive assumption: Arrival rate = completion rate ≈ {naive_lambda:.3f}\")\n",
        "print(\"Simulating a no-abandonment model with this lower rate...\")\n",
        "\n",
        "# Run replications of the misspecified model\n",
        "naive_runs = run_replications(\n",
        "    naive_cfg, n_rep=30, run_time=2000.0, warmup=200.0, seed=1234, patience_sampler=None\n",
        ")\n",
        "naive_mean_wait = naive_runs[\"mean_wait\"].mean()\n",
        "\n",
        "\n",
        "# --- 3. Comparison ---\n",
        "# The naive model, fed with a lower arrival rate, wrongly predicts a much lower waiting time.\n",
        "print(\"\\n--- 3. Comparison of Results ---\")\n",
        "print(f\"True System Mean Wait:      {true_metrics.mean_wait:.4f}\")\n",
        "print(f\"Naive Model Predicted Wait: {naive_mean_wait:.4f}\")\n",
        "print(f\"Prediction Error:           {(naive_mean_wait - true_metrics.mean_wait) / true_metrics.mean_wait:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3113b60a",
      "metadata": {},
      "source": [
        "### Interpreting the Misspecification Result\n",
        "- In the true system, some customers abandon, so completions underestimate the real arrival rate and hide lost demand.\n",
        "- A naive analyst who only sees completions and assumes infinite patience will think the system is less loaded and will mis-estimate waiting times.\n",
        "- Recommendations based on the naive model are over-optimistic: they ignore both lost customers and the increased congestion that would appear if everyone actually stayed.\n",
        "- Before adopting a no-abandonment model, always check the data for signs of reneging or timeouts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccfb342d",
      "metadata": {},
      "source": [
        "## Scenario Exploration (Synthetic)\n",
        "\n",
        "This section runs a small, synthetic scenario study to illustrate how capacity (number of servers) and demand scale interact and to quantify the resulting performance and uncertainty.\n",
        "\n",
        "What is done\n",
        "- We sweep a small grid of configurations: servers ∈ {base−1, base, base+1, base+2} and demand multipliers ∈ {0.8, 1.0, 1.2}.\n",
        "- For each configuration we run n_rep = 20 independent replications of the discrete‑event simulator using the run_time and warmup from BASE_PARAMS.\n",
        "- For each replication we collect mean waiting time (Wq), time‑average queue length (Lq), and utilisation; the code then reports the sample mean and a 95% CI for each metric via the summarize(...) helper.\n",
        "\n",
        "Why this is useful\n",
        "- Capacity planning and robustness: the grid shows how small changes in demand or servers move the system between under‑loaded, well‑operating, and near‑saturation regimes.\n",
        "- Detect regime shifts: when utilisation nears 1 the mean wait (and its uncertainty) typically explodes — this is important to spot before making operational decisions.\n",
        "- Decision support under uncertainty: the CIs help assess whether differences between configurations are meaningful (statistical vs. simulation noise), and enable cost/benefit trade‑offs when paired with a cost model.\n",
        "\n",
        "What to look for in the output\n",
        "- scenarios (DataFrame): one row per configuration with columns servers, demand_mult, mean_wait, mean_wait_lo, mean_wait_hi, lq, lq_lo, lq_hi, util, util_lo, util_hi. Use these to compare performance and uncertainty across configs.\n",
        "- cost_table (DataFrame, built in the next cell): maps each scenario to a simple cost = c_server * servers + c_wait * λ * mean_wait so you can rank configurations by expected cost.\n",
        "- Interpretation tips: very large mean_wait (and large CI) indicates a configuration that is effectively unstable for the chosen demand; tight CIs indicate reliable simulation estimates; compare cost_table to trade off staffing cost vs. customer waiting cost.\n",
        "\n",
        "Use these results to choose feasible configurations (satisfying SLA or service‑level constraints) or to prioritise further experiments (e.g., finer demand grid, alternative cost weights, or exploring patience/abandonment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472f5244",
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize(series: pd.Series) -> Tuple[float, float, float]:\n",
        "    x = series.dropna().values\n",
        "    m = x.mean()\n",
        "    s = x.std(ddof=1)\n",
        "    half = 1.96 * s / np.sqrt(len(x))\n",
        "    return m, m - half, m + half\n",
        "\n",
        "base_c = BASE_PARAMS[\"servers\"]\n",
        "servers = sorted({max(1, base_c - 1), base_c, base_c + 1, base_c + 2})\n",
        "demand_multipliers = [0.8, 1.0, 1.2]\n",
        "\n",
        "rows = []\n",
        "for c in servers:\n",
        "    for alpha in demand_multipliers:\n",
        "        cfg = {**BASE_PARAMS, \"lambda_rate\": BASE_PARAMS[\"lambda_rate\"] * alpha, \"servers\": c}\n",
        "        runs = run_replications(cfg, n_rep=20, run_time=BASE_PARAMS['run_time'], warmup=BASE_PARAMS['warmup'], seed=100)\n",
        "        mw = summarize(runs[\"mean_wait\"])\n",
        "        lq = summarize(runs[\"lq_timeavg\"])\n",
        "        ut = summarize(runs[\"utilisation\"])\n",
        "        rows.append({\n",
        "            \"servers\": c,\n",
        "            \"demand_mult\": alpha,\n",
        "            \"mean_wait\": mw[0], \"mean_wait_lo\": mw[1], \"mean_wait_hi\": mw[2],\n",
        "            \"lq\": lq[0], \"lq_lo\": lq[1], \"lq_hi\": lq[2],\n",
        "            \"util\": ut[0], \"util_lo\": ut[1], \"util_hi\": ut[2],\n",
        "        })\n",
        "\n",
        "scenarios = pd.DataFrame(rows)\n",
        "scenarios\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10a3e82",
      "metadata": {},
      "source": [
        "### Optional Cost Comparison\n",
        "\n",
        "Translate performance into cost if desired; adjust weights as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5e211c",
      "metadata": {},
      "outputs": [],
      "source": [
        "c_server = 1.0   # cost per server per unit time\n",
        "c_wait = 5.0     # cost per unit waiting time per job\n",
        "\n",
        "cost_rows = []\n",
        "for _, row in scenarios.iterrows():\n",
        "    lam = BASE_PARAMS[\"lambda_rate\"] * row[\"demand_mult\"]\n",
        "    cost_mean = c_server * row[\"servers\"] + c_wait * lam * row[\"mean_wait\"]\n",
        "    cost_lo = c_server * row[\"servers\"] + c_wait * lam * row[\"mean_wait_lo\"]\n",
        "    cost_hi = c_server * row[\"servers\"] + c_wait * lam * row[\"mean_wait_hi\"]\n",
        "    cost_rows.append({**row, \"cost\": cost_mean, \"cost_lo\": cost_lo, \"cost_hi\": cost_hi})\n",
        "\n",
        "cost_table = pd.DataFrame(cost_rows)\n",
        "cost_table.sort_values([\"demand_mult\", \"cost\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "341a1dcd",
      "metadata": {},
      "source": [
        "**Exercice:**  How many servers would you choose ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a303e02d",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "- Modify `BASE_PARAMS` and observe how stability/utilisation change; detect when the system nears saturation.\n",
        "- Extend the abandonment example: vary patience mean and quantify how the naive (no-abandonment) model mis-predicts waits.\n",
        "- Try a heavier-tailed service sampler (e.g. lognormal) and compare CIs to the shifted-exponential baseline.\n",
        "- Add a service-level constraint (e.g. target $P(W_q > w^*)$) and choose the cheapest feasible configuration.\n",
        "- Document which diagnostics justify your chosen model and which behaviours the model cannot capture."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venvLLMDS (3.13.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
